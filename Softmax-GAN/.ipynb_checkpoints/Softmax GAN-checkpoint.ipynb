{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d0f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446a0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # hyperparams\n",
    "    batch_size = 64\n",
    "    n_epochs = 100\n",
    "    input_shape = [1,28,28]  # in the case of mnist\n",
    "    G_lr = 0.0002\n",
    "    D_lr = 0.0002\n",
    "    G_betas = (0.5, 0.999)\n",
    "    D_betas = (0.5, 0.999)\n",
    "    # model params\n",
    "    n_latent = 100\n",
    "    G_n_blocks = 2\n",
    "    D_n_blocks = 3\n",
    "    G_dim = 64\n",
    "    D_dim = 64\n",
    "    \n",
    "    # device params\n",
    "    gpus = \"0\"\n",
    "    \n",
    "device = torch.device(f\"cuda:{args.gpus}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6063cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"/jupyterdata/\", download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5de5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, h, out_ch, n_latent, cur_dim, n_blocks = 2, k=3, s=1, p=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_dim = cur_dim\n",
    "        self.init_size = h // (2**n_blocks)  # n_layer번 업샘플링 되므로 처음 채널 사이즈 조절\n",
    "        self.linear = nn.Sequential(nn.Linear(n_latent, cur_dim * (self.init_size**2)))\n",
    "        layers = []\n",
    "        for _ in range(n_blocks):  # 한번 지날 수록 \n",
    "            layers.append(self.basic_block(cur_dim, cur_dim*2, k, s, p))\n",
    "            cur_dim *= 2\n",
    "            \n",
    "        layers.append(nn.Conv2d(cur_dim, out_ch, k,s,p))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "            \n",
    "    @staticmethod\n",
    "    def basic_block(in_ch, out_ch, k=3, s=1, p=1):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, k, s, p))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Upsample(scale_factor=2))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        x = self.linear(input_)\n",
    "        y = x.reshape(input_.shape[0], self.init_dim, self.init_size, self.init_size)\n",
    "        return self.layers(y)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_ch, cur_dim, n_blocks, h, k=3, s=2, p=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        feat_extract = []\n",
    "        feat_extract.append(self.basic_block(in_ch, cur_dim))\n",
    "        for _ in range(n_blocks-1):\n",
    "            feat_extract.append(self.basic_block(cur_dim, cur_dim*2, k, s, p))\n",
    "            cur_dim *= 2\n",
    "        \n",
    "        feat_extract.append(nn.AdaptiveAvgPool2d((1,1)))\n",
    "        self.feat_extract = nn.Sequential(*feat_extract)\n",
    "        self.linear = nn.Sequential(nn.Linear(cur_dim, 1))\n",
    "        \n",
    "    @staticmethod\n",
    "    def basic_block(in_ch, out_ch, k=3, s=2, p=1):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, k, s, p))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        x = self.feat_extract(input_)\n",
    "        y = x.reshape(input_.shape[0], -1)\n",
    "        return self.linear(y)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8119d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(args.input_shape[1], args.input_shape[0], args.n_latent, args.G_dim, args.G_n_blocks).to(device)\n",
    "D = Discriminator(args.input_shape[0], args.D_dim, args.D_n_blocks, args.input_shape[1]).to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=args.G_lr, betas=args.G_betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=args.D_lr, betas=args.D_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87fe433a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D loss : 4.313283920288086, G loss : 5.561254501342773\n",
      "D loss : 4.373970985412598, G loss : 5.46267557144165\n",
      "D loss : 4.399170398712158, G loss : 5.427210807800293\n",
      "D loss : 4.482545852661133, G loss : 5.1585540771484375\n",
      "D loss : 4.478521347045898, G loss : 5.238565444946289\n",
      "D loss : 4.585352897644043, G loss : 5.1282830238342285\n",
      "D loss : 4.5654449462890625, G loss : 5.094207763671875\n",
      "D loss : 4.617183208465576, G loss : 5.156890869140625\n",
      "D loss : 4.509515762329102, G loss : 5.138433456420898\n",
      "D loss : 4.490675926208496, G loss : 5.168750762939453\n",
      "D loss : 4.52320671081543, G loss : 5.209141254425049\n",
      "D loss : 4.552839279174805, G loss : 5.0676984786987305\n",
      "D loss : 4.503279685974121, G loss : 5.255790710449219\n",
      "D loss : 4.509060382843018, G loss : 5.199960231781006\n",
      "D loss : 4.540794849395752, G loss : 5.219198226928711\n",
      "D loss : 4.444958686828613, G loss : 5.24819803237915\n",
      "D loss : 4.452409744262695, G loss : 5.297887325286865\n",
      "D loss : 4.526363849639893, G loss : 5.183923721313477\n",
      "D loss : 4.444667339324951, G loss : 5.400562286376953\n",
      "D loss : 4.571784973144531, G loss : 5.099502086639404\n",
      "D loss : 4.4725799560546875, G loss : 5.323246002197266\n",
      "D loss : 4.423533916473389, G loss : 5.385195255279541\n",
      "D loss : 4.4868621826171875, G loss : 5.274366855621338\n",
      "D loss : 4.464449882507324, G loss : 5.345232963562012\n",
      "D loss : 4.323067665100098, G loss : 5.493279457092285\n",
      "D loss : 4.405788421630859, G loss : 5.209101676940918\n",
      "D loss : 4.460504531860352, G loss : 5.273584365844727\n",
      "D loss : 4.542452812194824, G loss : 5.5204362869262695\n",
      "D loss : 4.427958011627197, G loss : 5.305643558502197\n",
      "D loss : 4.3421454429626465, G loss : 5.525385856628418\n",
      "D loss : 4.336275100708008, G loss : 5.501642227172852\n",
      "D loss : 4.446653842926025, G loss : 5.5927042961120605\n",
      "D loss : 4.303257465362549, G loss : 5.353296279907227\n",
      "D loss : 4.282161235809326, G loss : 6.218393802642822\n",
      "D loss : 4.34656286239624, G loss : 5.412839889526367\n",
      "D loss : 4.453036785125732, G loss : 5.65235710144043\n",
      "D loss : 4.540706157684326, G loss : 5.148637771606445\n",
      "D loss : 4.231956481933594, G loss : 5.805294513702393\n",
      "D loss : 4.265774250030518, G loss : 5.787477016448975\n",
      "D loss : 4.496184349060059, G loss : 5.5467424392700195\n",
      "D loss : 4.2142415046691895, G loss : 6.590189456939697\n",
      "D loss : 4.29449987411499, G loss : 5.8582658767700195\n",
      "D loss : 4.206045150756836, G loss : 7.275844573974609\n",
      "D loss : 4.25719690322876, G loss : 5.960663318634033\n",
      "D loss : 4.233206272125244, G loss : 5.825356960296631\n",
      "D loss : 4.226212501525879, G loss : 5.901966094970703\n",
      "D loss : 4.302131652832031, G loss : 5.651515960693359\n",
      "D loss : 4.200042724609375, G loss : 6.422221660614014\n",
      "D loss : 4.194702625274658, G loss : 6.3750762939453125\n",
      "D loss : 4.20923376083374, G loss : 5.988327980041504\n",
      "D loss : 4.204035758972168, G loss : 7.113336563110352\n",
      "D loss : 4.244408130645752, G loss : 5.949994087219238\n",
      "D loss : 4.241946697235107, G loss : 7.323973655700684\n",
      "D loss : 4.50226354598999, G loss : 6.226223945617676\n",
      "D loss : 4.418883800506592, G loss : 5.2884111404418945\n",
      "D loss : 4.260788917541504, G loss : 6.303956031799316\n",
      "D loss : 4.577635765075684, G loss : 5.314914703369141\n",
      "D loss : 4.22063684463501, G loss : 7.023682594299316\n",
      "D loss : 4.189863204956055, G loss : 8.901469230651855\n",
      "D loss : 4.416226863861084, G loss : 5.339652061462402\n",
      "D loss : 4.229402542114258, G loss : 8.088278770446777\n",
      "D loss : 4.249234676361084, G loss : 5.755433082580566\n",
      "D loss : 4.27848482131958, G loss : 5.727161407470703\n",
      "D loss : 4.23190975189209, G loss : 6.06593132019043\n",
      "D loss : 4.237887859344482, G loss : 6.777864456176758\n",
      "D loss : 4.458790302276611, G loss : 6.21205997467041\n",
      "D loss : 4.235185146331787, G loss : 6.227075099945068\n",
      "D loss : 4.239984512329102, G loss : 5.853326797485352\n",
      "D loss : 4.223834037780762, G loss : 6.211178302764893\n",
      "D loss : 4.204219818115234, G loss : 8.160449028015137\n",
      "D loss : 4.204290390014648, G loss : 6.516013145446777\n",
      "D loss : 4.204357624053955, G loss : 6.7383036613464355\n",
      "D loss : 4.218643665313721, G loss : 9.659954071044922\n",
      "D loss : 4.269582748413086, G loss : 5.7470502853393555\n",
      "D loss : 4.214698791503906, G loss : 8.084035873413086\n",
      "D loss : 4.208459377288818, G loss : 6.630889892578125\n",
      "D loss : 4.277784824371338, G loss : 5.798237323760986\n",
      "D loss : 4.185150146484375, G loss : 8.492013931274414\n",
      "D loss : 4.192747592926025, G loss : 9.312154769897461\n",
      "D loss : 4.4317851066589355, G loss : 5.480437278747559\n",
      "D loss : 4.328120231628418, G loss : 6.054601669311523\n",
      "D loss : 4.225491046905518, G loss : 6.213407516479492\n",
      "D loss : 4.227019786834717, G loss : 6.129005432128906\n",
      "D loss : 4.192848205566406, G loss : 10.00049114227295\n",
      "D loss : 4.186114311218262, G loss : 6.970785617828369\n",
      "D loss : 4.324967384338379, G loss : 5.635254859924316\n",
      "D loss : 4.189558982849121, G loss : 8.383838653564453\n",
      "D loss : 4.193037986755371, G loss : 11.444812774658203\n",
      "D loss : 4.170486927032471, G loss : 11.729177474975586\n",
      "D loss : 4.231618404388428, G loss : 6.6164231300354\n",
      "D loss : 4.181496620178223, G loss : 10.73088550567627\n",
      "D loss : 4.18004846572876, G loss : 9.27053165435791\n",
      "D loss : 4.189637184143066, G loss : 9.950664520263672\n",
      "D loss : 4.191673278808594, G loss : 8.845691680908203\n",
      "D loss : 4.184746742248535, G loss : 10.166875839233398\n",
      "D loss : 4.2393574714660645, G loss : 6.480663299560547\n",
      "D loss : 4.187268257141113, G loss : 11.685501098632812\n",
      "D loss : 4.193594932556152, G loss : 9.1904935836792\n",
      "D loss : 4.1811017990112305, G loss : 7.925647735595703\n",
      "D loss : 4.232283592224121, G loss : 6.891977310180664\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    for img, target in train_loader:\n",
    "        img = img.to(device)\n",
    "        \n",
    "        B = 1/(args.batch_size*2)\n",
    "        \n",
    "        latent_z = torch.FloatTensor(np.random.randn(args.batch_size, args.n_latent)).to(device)\n",
    "        gene_img = G(latent_z)\n",
    "        \n",
    "        # training D\n",
    "        real_logit = D(img)\n",
    "        gene_logit = D(gene_img.detach())\n",
    "        Z_B = torch.sum(torch.exp(-real_logit)) + torch.sum(torch.exp(-gene_logit))  # Z_B 는 모든 배치에 대하여 구하는 것.\n",
    "        D_loss = torch.mean(real_logit) + torch.log(Z_B) \n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        D_loss.backward() \n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # training G\n",
    "        real_logit = D(img)\n",
    "        gene_logit = D(gene_img)\n",
    "        Z_B = torch.sum(torch.exp(-real_logit)) + torch.sum(torch.exp(-gene_logit))\n",
    "        G_loss = torch.sum(real_logit) * B + torch.sum(gene_logit) * B + torch.log(Z_B)\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    if not os.path.isdir(\"./generated_images\"):\n",
    "        os.makedirs(\"./generated_images\")\n",
    "    torchvision.utils.save_image(gene_img, \"./generated_images/{}_2.jpg\".format(epoch))\n",
    "    print(f\"D loss : {D_loss}, G loss : {G_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df4dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(args.input_shape[1], args.input_shape[0], args.n_latent, args.G_dim, args.G_n_blocks).to(device)\n",
    "D = Discriminator(args.input_shape[0], args.D_dim, args.D_n_blocks, args.input_shape[1]).to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=args.G_lr, betas=args.G_betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=args.D_lr, betas=args.D_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f52ac140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D loss : 4.406492233276367, G loss : 5.301234245300293\n",
      "D loss : 4.547021865844727, G loss : 5.188363552093506\n",
      "D loss : 4.535688400268555, G loss : 5.119220733642578\n",
      "D loss : 4.535009860992432, G loss : 5.125333786010742\n",
      "D loss : 4.527644157409668, G loss : 5.145512580871582\n",
      "D loss : 4.574830055236816, G loss : 5.09554386138916\n",
      "D loss : 4.540482044219971, G loss : 5.106075763702393\n",
      "D loss : 4.496939182281494, G loss : 5.200570106506348\n",
      "D loss : 4.560903549194336, G loss : 5.210740089416504\n",
      "D loss : 4.542243003845215, G loss : 5.137052536010742\n",
      "D loss : 4.533084392547607, G loss : 5.170156002044678\n",
      "D loss : 4.466892242431641, G loss : 5.255204200744629\n",
      "D loss : 4.511850357055664, G loss : 5.226292610168457\n",
      "D loss : 4.432041168212891, G loss : 5.227130889892578\n",
      "D loss : 4.387894153594971, G loss : 5.350286483764648\n",
      "D loss : 4.448821067810059, G loss : 5.2919921875\n",
      "D loss : 4.418593406677246, G loss : 5.247453689575195\n",
      "D loss : 4.4138617515563965, G loss : 5.396124839782715\n",
      "D loss : 4.347701072692871, G loss : 5.479917526245117\n",
      "D loss : 4.561842918395996, G loss : 5.304231643676758\n",
      "D loss : 4.306003093719482, G loss : 6.069212436676025\n",
      "D loss : 4.311481952667236, G loss : 5.578519821166992\n",
      "D loss : 4.2487077713012695, G loss : 5.70599889755249\n",
      "D loss : 4.278218746185303, G loss : 6.061695098876953\n",
      "D loss : 4.278158187866211, G loss : 5.8321967124938965\n",
      "D loss : 4.423346042633057, G loss : 5.248843193054199\n",
      "D loss : 4.229283332824707, G loss : 6.272533416748047\n",
      "D loss : 4.375537395477295, G loss : 5.318683624267578\n",
      "D loss : 4.249761581420898, G loss : 5.9549360275268555\n",
      "D loss : 4.266763210296631, G loss : 5.945501327514648\n",
      "D loss : 4.567817211151123, G loss : 5.099910736083984\n",
      "D loss : 4.663928031921387, G loss : 5.46558952331543\n",
      "D loss : 4.305694580078125, G loss : 5.447386264801025\n",
      "D loss : 4.584298133850098, G loss : 5.874953269958496\n",
      "D loss : 5.060454368591309, G loss : 6.588367462158203\n",
      "D loss : 4.241272449493408, G loss : 7.371793270111084\n",
      "D loss : 4.456557273864746, G loss : 5.200033187866211\n",
      "D loss : 4.228089332580566, G loss : 6.009200096130371\n",
      "D loss : 4.231973648071289, G loss : 6.079548358917236\n",
      "D loss : 4.2056498527526855, G loss : 6.81075382232666\n",
      "D loss : 4.2325568199157715, G loss : 7.455754280090332\n",
      "D loss : 4.258199214935303, G loss : 5.564792633056641\n",
      "D loss : 4.612423896789551, G loss : 5.102674961090088\n",
      "D loss : 4.201003551483154, G loss : 7.157234191894531\n",
      "D loss : 4.203746795654297, G loss : 7.492323875427246\n",
      "D loss : 4.196634292602539, G loss : 6.549896717071533\n",
      "D loss : 4.253702640533447, G loss : 6.044966220855713\n",
      "D loss : 4.191802024841309, G loss : 6.844996452331543\n",
      "D loss : 4.377445220947266, G loss : 5.349425315856934\n",
      "D loss : 4.213966369628906, G loss : 6.76536750793457\n",
      "D loss : 4.3258442878723145, G loss : 5.432372570037842\n",
      "D loss : 4.336585998535156, G loss : 6.155122756958008\n",
      "D loss : 4.304524898529053, G loss : 5.638727188110352\n",
      "D loss : 4.201471328735352, G loss : 8.556001663208008\n",
      "D loss : 4.246896266937256, G loss : 5.781956672668457\n",
      "D loss : 4.233309745788574, G loss : 6.035619735717773\n",
      "D loss : 4.246223449707031, G loss : 6.039912223815918\n",
      "D loss : 4.240999221801758, G loss : 5.855820655822754\n",
      "D loss : 4.815970420837402, G loss : 5.080165863037109\n",
      "D loss : 4.200676918029785, G loss : 7.346642971038818\n",
      "D loss : 4.246664047241211, G loss : 6.094046592712402\n",
      "D loss : 4.206235408782959, G loss : 7.4602861404418945\n",
      "D loss : 4.236343860626221, G loss : 5.985813617706299\n",
      "D loss : 4.3206024169921875, G loss : 5.491962432861328\n",
      "D loss : 4.268496990203857, G loss : 5.710361480712891\n",
      "D loss : 4.421919822692871, G loss : 5.364805698394775\n",
      "D loss : 4.208835601806641, G loss : 8.45772933959961\n",
      "D loss : 4.422774314880371, G loss : 5.295207977294922\n",
      "D loss : 4.213420867919922, G loss : 6.429786682128906\n",
      "D loss : 4.240777969360352, G loss : 6.086001396179199\n",
      "D loss : 4.226495742797852, G loss : 8.213529586791992\n",
      "D loss : 4.211835861206055, G loss : 7.957575798034668\n",
      "D loss : 4.466907501220703, G loss : 5.334883689880371\n",
      "D loss : 4.253986835479736, G loss : 6.08863639831543\n",
      "D loss : 4.225907802581787, G loss : 5.948936462402344\n",
      "D loss : 4.327981948852539, G loss : 5.461840629577637\n",
      "D loss : 4.3068928718566895, G loss : 5.484055519104004\n",
      "D loss : 4.2114949226379395, G loss : 6.133059501647949\n",
      "D loss : 4.222438812255859, G loss : 7.405302047729492\n",
      "D loss : 4.1988091468811035, G loss : 8.236448287963867\n",
      "D loss : 4.262318134307861, G loss : 6.012484550476074\n",
      "D loss : 4.389927864074707, G loss : 5.446927547454834\n",
      "D loss : 4.271267414093018, G loss : 5.689264297485352\n",
      "D loss : 4.370039939880371, G loss : 5.4116668701171875\n",
      "D loss : 4.218830585479736, G loss : 7.880107879638672\n",
      "D loss : 4.205557346343994, G loss : 6.478479862213135\n",
      "D loss : 4.1905293464660645, G loss : 6.972482681274414\n",
      "D loss : 4.237676620483398, G loss : 7.618804931640625\n",
      "D loss : 4.196298122406006, G loss : 7.585594654083252\n",
      "D loss : 4.257286071777344, G loss : 6.28572416305542\n",
      "D loss : 4.413639068603516, G loss : 5.475709438323975\n",
      "D loss : 4.193486213684082, G loss : 6.516933917999268\n",
      "D loss : 4.343442440032959, G loss : 5.450530052185059\n",
      "D loss : 4.279183387756348, G loss : 5.728421688079834\n",
      "D loss : 4.189084053039551, G loss : 7.7849626541137695\n",
      "D loss : 4.2038421630859375, G loss : 8.668834686279297\n",
      "D loss : 4.209453582763672, G loss : 6.624197483062744\n",
      "D loss : 4.2536420822143555, G loss : 5.7512407302856445\n",
      "D loss : 4.17950439453125, G loss : 9.378114700317383\n",
      "D loss : 4.222517013549805, G loss : 6.181893348693848\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    for img, target in train_loader:\n",
    "        img = img.to(device)\n",
    "        \n",
    "        B = 1/(args.batch_size*2)\n",
    "        \n",
    "        latent_z = torch.FloatTensor(np.random.randn(args.batch_size, args.n_latent)).to(device)\n",
    "        gene_img = G(latent_z)\n",
    "        \n",
    "        # training D\n",
    "        real_logit = D(img)\n",
    "        gene_logit = D(gene_img)\n",
    "        Z_B = torch.sum(torch.exp(-real_logit)) + torch.sum(torch.exp(-gene_logit))  # Z_B 는 모든 배치에 대하여 구하는 것.\n",
    "        D_loss = torch.mean(real_logit) + torch.log(Z_B) \n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        D_loss.backward(retain_graph=True) \n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # training G\n",
    "        real_logit = D(img)\n",
    "        gene_logit = D(gene_img)\n",
    "        Z_B = torch.sum(torch.exp(-real_logit)) + torch.sum(torch.exp(-gene_logit))\n",
    "        G_loss = torch.sum(real_logit) * B + torch.sum(gene_logit) * B + torch.log(Z_B)\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    if not os.path.isdir(\"./generated_images\"):\n",
    "        os.makedirs(\"./generated_images\")\n",
    "    torchvision.utils.save_image(gene_img, \"./generated_images/{}_2.jpg\".format(epoch))\n",
    "    print(f\"D loss : {D_loss}, G loss : {G_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd92a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(args.input_shape[1], args.input_shape[0], args.n_latent, args.G_dim, args.G_n_blocks).to(device)\n",
    "D = Discriminator(args.input_shape[0], args.D_dim, args.D_n_blocks, args.input_shape[1]).to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=args.G_lr, betas=args.G_betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=args.D_lr, betas=args.D_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ae691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D loss : 4.509868621826172, G loss : 4.973240375518799\n",
      "D loss : 4.5428056716918945, G loss : 4.943559646606445\n",
      "D loss : 4.536262035369873, G loss : 4.951693534851074\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    for idx, (img, target) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        B = 1/(args.batch_size*2)\n",
    "        latent_z = torch.FloatTensor(np.random.randn(args.batch_size, args.n_latent)).to(device)\n",
    "        \n",
    "        gene_img = G(latent_z)\n",
    "        \n",
    "        real_logit = D(img)\n",
    "        gene_logit = D(gene_img)\n",
    "        Z_B = torch.sum(torch.exp(-real_logit)) + torch.sum(torch.exp(-gene_logit))  # Z_B 는 모든 배치에 대하여 구하는 것.\n",
    "        D_loss = torch.mean(real_logit) + torch.log(Z_B) \n",
    "        G_loss = torch.sum(real_logit) * B + torch.sum(gene_logit) * B + torch.log(Z_B)\n",
    "        if idx % 2 == 1: # 한번은 G, D 모두 훈련\n",
    "            optimizer_D.zero_grad()\n",
    "            optimizer_G.zero_grad()\n",
    "            D_loss.backward(retain_graph=True) \n",
    "            G_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            optimizer_G.step()\n",
    "        if idx % 2 == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    if not os.path.isdir(\"./generated_images\"):\n",
    "        os.makedirs(\"./generated_images\")\n",
    "    torchvision.utils.save_image(gene_img, \"./generated_images/{}_3.jpg\".format(epoch))\n",
    "    print(f\"D loss : {D_loss}, G loss : {G_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a2660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a565380",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    for idx, (img, target) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        B = 1/(args.batch_size*2)\n",
    "        latent_z = torch.FloatTensor(np.random.randn(args.batch_size, args.n_latent)).to(device)\n",
    "        \n",
    "        gene_img = G(latent_z)\n",
    "        \n",
    "        real_logit = D(img)\n",
    "        gene_logit = D(gene_img)\n",
    "        Z_B = torch.sum(torch.exp(-real_logit)) + torch.sum(torch.exp(-gene_logit))  # Z_B 는 모든 배치에 대하여 구하는 것.\n",
    "        D_loss = torch.mean(real_logit) + torch.log(Z_B) \n",
    "        G_loss = torch.sum(real_logit) * B + torch.sum(gene_logit) * B + torch.log(Z_B)\n",
    "        optimizer_D.zero_grad()\n",
    "        D_loss.backward(retain_graph=True) \n",
    "        optimizer_D.step()\n",
    "        optimizer_G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    if not os.path.isdir(\"./generated_images\"):\n",
    "        os.makedirs(\"./generated_images\")\n",
    "    torchvision.utils.save_image(gene_img, \"./generated_images/{}_3.jpg\".format(epoch))\n",
    "    print(f\"D loss : {D_loss}, G loss : {G_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4979d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0472c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05dcf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0bf69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GANs",
   "language": "python",
   "name": "gans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
