{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "INPUT_SIZE = 28\n",
    "INPUT_SHAPE = (1,INPUT_SIZE, INPUT_SIZE)\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 200\n",
    "BETAS = (0.5, 0.999)\n",
    "LATENT_DIM = 100\n",
    "N_CRITIC = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../data\", download = True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle = True, drop_last = True, num_workers = 4, pin_memory = True, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, dim_list):\n",
    "        super(Generator, self).__init__()\n",
    "        blocks = [nn.Linear(latent_dim, dim_list[0]), nn.LeakyReLU(0.2)]\n",
    "        for in_ch, out_ch in zip(dim_list[:-1], dim_list[1:]):\n",
    "            blocks.extend(self._basic_block(in_ch, out_ch))\n",
    "        blocks.append(nn.Linear(dim_list[-1], int(np.prod(INPUT_SHAPE))))\n",
    "        blocks.append(nn.Tanh())\n",
    "        self.generator = nn.Sequential(*blocks)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def _basic_block(self, in_channels, out_channels, bn=True):\n",
    "        block = [nn.Linear(in_channels, out_channels)]\n",
    "        if bn:\n",
    "            block.append(nn.BatchNorm1d(out_channels))\n",
    "        block.append(nn.LeakyReLU(0.2))\n",
    "        return block\n",
    "\n",
    "    def forward(self, z):\n",
    "        output = self.generator(z)  # output => [BATCH_SIZE x CHANNELS x W x H]\n",
    "        return output.reshape(output.shape[0], *INPUT_SHAPE)\n",
    "        \n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(INPUT_SHAPE)), 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512,256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_img):\n",
    "        input_flat = input_img.reshape(input_img.shape[0], -1)\n",
    "        return self.discriminator(input_flat)  # return => [BATCH_SIZE x 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator(LATENT_DIM, [128,256,512, 1024]).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr = LEARNING_RATE, betas=BETAS)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr = LEARNING_RATE, betas=BETAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : [0/200], BATCH : [0/234], loss D : -0.04032719135284424, loss G : -0.05101131647825241\n",
      "EPOCH : [0/200], BATCH : [100/234], loss D : -0.027008056640625, loss G : -18.430889129638672\n",
      "EPOCH : [0/200], BATCH : [200/234], loss D : 0.22512054443359375, loss G : -18.48990249633789\n",
      "EPOCH : [1/200], BATCH : [0/234], loss D : -0.6034603118896484, loss G : -17.316078186035156\n",
      "EPOCH : [1/200], BATCH : [100/234], loss D : -0.08206558227539062, loss G : -18.35706901550293\n",
      "EPOCH : [1/200], BATCH : [200/234], loss D : -0.05511665344238281, loss G : -15.328865051269531\n",
      "EPOCH : [2/200], BATCH : [0/234], loss D : -0.3961372375488281, loss G : -14.471179962158203\n",
      "EPOCH : [2/200], BATCH : [100/234], loss D : -0.08515167236328125, loss G : -12.708065032958984\n",
      "EPOCH : [2/200], BATCH : [200/234], loss D : -0.12046432495117188, loss G : -18.434553146362305\n",
      "EPOCH : [3/200], BATCH : [0/234], loss D : -0.32100963592529297, loss G : -13.183809280395508\n",
      "EPOCH : [3/200], BATCH : [100/234], loss D : -0.18707275390625, loss G : -15.79662799835205\n",
      "EPOCH : [3/200], BATCH : [200/234], loss D : -0.2175445556640625, loss G : -11.497997283935547\n",
      "EPOCH : [4/200], BATCH : [0/234], loss D : -0.027894973754882812, loss G : -19.433446884155273\n",
      "EPOCH : [4/200], BATCH : [100/234], loss D : -0.27918052673339844, loss G : -13.378969192504883\n",
      "EPOCH : [4/200], BATCH : [200/234], loss D : -0.35956573486328125, loss G : -17.203842163085938\n",
      "EPOCH : [5/200], BATCH : [0/234], loss D : -0.3969745635986328, loss G : -15.816535949707031\n",
      "EPOCH : [5/200], BATCH : [100/234], loss D : 0.3421778678894043, loss G : -8.354896545410156\n",
      "EPOCH : [5/200], BATCH : [200/234], loss D : 0.7431116104125977, loss G : -8.484928131103516\n",
      "EPOCH : [6/200], BATCH : [0/234], loss D : 0.8096561431884766, loss G : -21.47687339782715\n",
      "EPOCH : [6/200], BATCH : [100/234], loss D : 0.14078521728515625, loss G : -19.629575729370117\n",
      "EPOCH : [6/200], BATCH : [200/234], loss D : -0.4843482971191406, loss G : -12.008102416992188\n",
      "EPOCH : [7/200], BATCH : [0/234], loss D : -0.23607826232910156, loss G : -26.777217864990234\n",
      "EPOCH : [7/200], BATCH : [100/234], loss D : -0.222991943359375, loss G : -26.78607940673828\n",
      "EPOCH : [7/200], BATCH : [200/234], loss D : -1.1077461242675781, loss G : -24.00221824645996\n",
      "EPOCH : [8/200], BATCH : [0/234], loss D : -0.23564529418945312, loss G : -22.299821853637695\n",
      "EPOCH : [8/200], BATCH : [100/234], loss D : -1.5054168701171875, loss G : -18.61743927001953\n",
      "EPOCH : [8/200], BATCH : [200/234], loss D : 0.39989471435546875, loss G : -39.590274810791016\n",
      "EPOCH : [9/200], BATCH : [0/234], loss D : 0.18646621704101562, loss G : -25.488544464111328\n",
      "EPOCH : [9/200], BATCH : [100/234], loss D : -1.2189865112304688, loss G : -26.247943878173828\n",
      "EPOCH : [9/200], BATCH : [200/234], loss D : 0.0854644775390625, loss G : -39.659873962402344\n",
      "EPOCH : [10/200], BATCH : [0/234], loss D : -0.9729652404785156, loss G : -32.99225997924805\n",
      "EPOCH : [10/200], BATCH : [100/234], loss D : -0.8464736938476562, loss G : -30.776756286621094\n",
      "EPOCH : [10/200], BATCH : [200/234], loss D : -0.3312225341796875, loss G : -70.79405975341797\n",
      "EPOCH : [11/200], BATCH : [0/234], loss D : -2.6981277465820312, loss G : -37.96596145629883\n",
      "EPOCH : [11/200], BATCH : [100/234], loss D : -2.008617401123047, loss G : -37.760047912597656\n",
      "EPOCH : [11/200], BATCH : [200/234], loss D : -1.11065673828125, loss G : -85.33909606933594\n",
      "EPOCH : [12/200], BATCH : [0/234], loss D : -3.8934326171875, loss G : -93.1257553100586\n",
      "EPOCH : [12/200], BATCH : [100/234], loss D : -1.5367431640625, loss G : -131.353515625\n",
      "EPOCH : [12/200], BATCH : [200/234], loss D : 0.9025115966796875, loss G : -135.83392333984375\n",
      "EPOCH : [13/200], BATCH : [0/234], loss D : 2.9353790283203125, loss G : -109.29339599609375\n",
      "EPOCH : [13/200], BATCH : [100/234], loss D : 2.2447662353515625, loss G : -121.48129272460938\n",
      "EPOCH : [13/200], BATCH : [200/234], loss D : -5.169708251953125, loss G : -98.4073486328125\n",
      "EPOCH : [14/200], BATCH : [0/234], loss D : -5.10400390625, loss G : -101.29312133789062\n",
      "EPOCH : [14/200], BATCH : [100/234], loss D : -1.8526153564453125, loss G : -103.21221923828125\n",
      "EPOCH : [14/200], BATCH : [200/234], loss D : -1.438690185546875, loss G : -133.0980224609375\n",
      "EPOCH : [15/200], BATCH : [0/234], loss D : -4.71630859375, loss G : -145.80274963378906\n",
      "EPOCH : [15/200], BATCH : [100/234], loss D : -1.4675140380859375, loss G : -108.77293395996094\n",
      "EPOCH : [15/200], BATCH : [200/234], loss D : -0.12603759765625, loss G : -97.82925415039062\n",
      "EPOCH : [16/200], BATCH : [0/234], loss D : -4.644622802734375, loss G : -98.53572082519531\n",
      "EPOCH : [16/200], BATCH : [100/234], loss D : -2.9397659301757812, loss G : -108.23535919189453\n",
      "EPOCH : [16/200], BATCH : [200/234], loss D : 0.5799026489257812, loss G : -120.23715209960938\n",
      "EPOCH : [17/200], BATCH : [0/234], loss D : -5.95196533203125, loss G : -127.66675567626953\n",
      "EPOCH : [17/200], BATCH : [100/234], loss D : -1.135528564453125, loss G : -168.5245361328125\n",
      "EPOCH : [17/200], BATCH : [200/234], loss D : -2.774078369140625, loss G : -172.1517791748047\n",
      "EPOCH : [18/200], BATCH : [0/234], loss D : -4.7414093017578125, loss G : -181.12252807617188\n",
      "EPOCH : [18/200], BATCH : [100/234], loss D : -2.02880859375, loss G : -221.10165405273438\n",
      "EPOCH : [18/200], BATCH : [200/234], loss D : -7.1308441162109375, loss G : -214.82232666015625\n",
      "EPOCH : [19/200], BATCH : [0/234], loss D : -3.293060302734375, loss G : -209.92752075195312\n",
      "EPOCH : [19/200], BATCH : [100/234], loss D : -7.6958770751953125, loss G : -189.52590942382812\n",
      "EPOCH : [19/200], BATCH : [200/234], loss D : -1.02691650390625, loss G : -281.94488525390625\n",
      "EPOCH : [20/200], BATCH : [0/234], loss D : 0.610931396484375, loss G : -248.55325317382812\n",
      "EPOCH : [20/200], BATCH : [100/234], loss D : 3.22015380859375, loss G : -259.9058532714844\n",
      "EPOCH : [20/200], BATCH : [200/234], loss D : -1.53741455078125, loss G : -273.4836120605469\n",
      "EPOCH : [21/200], BATCH : [0/234], loss D : -0.0365142822265625, loss G : -251.79248046875\n",
      "EPOCH : [21/200], BATCH : [100/234], loss D : -3.498382568359375, loss G : -189.72018432617188\n",
      "EPOCH : [21/200], BATCH : [200/234], loss D : -4.0048980712890625, loss G : -237.45925903320312\n",
      "EPOCH : [22/200], BATCH : [0/234], loss D : -9.00433349609375, loss G : -311.97296142578125\n",
      "EPOCH : [22/200], BATCH : [100/234], loss D : -0.0352935791015625, loss G : -248.47113037109375\n",
      "EPOCH : [22/200], BATCH : [200/234], loss D : -5.6839599609375, loss G : -258.614501953125\n",
      "EPOCH : [23/200], BATCH : [0/234], loss D : -3.58740234375, loss G : -254.8209228515625\n",
      "EPOCH : [23/200], BATCH : [100/234], loss D : -0.8463134765625, loss G : -322.4315490722656\n",
      "EPOCH : [23/200], BATCH : [200/234], loss D : -5.7349090576171875, loss G : -202.7085723876953\n",
      "EPOCH : [24/200], BATCH : [0/234], loss D : 0.71099853515625, loss G : -297.59368896484375\n",
      "EPOCH : [24/200], BATCH : [100/234], loss D : -3.4682464599609375, loss G : -237.24407958984375\n",
      "EPOCH : [24/200], BATCH : [200/234], loss D : -1.07159423828125, loss G : -291.17205810546875\n",
      "EPOCH : [25/200], BATCH : [0/234], loss D : -0.22283935546875, loss G : -296.7086181640625\n",
      "EPOCH : [25/200], BATCH : [100/234], loss D : -1.338623046875, loss G : -318.94378662109375\n",
      "EPOCH : [25/200], BATCH : [200/234], loss D : -8.536865234375, loss G : -447.7010192871094\n",
      "EPOCH : [26/200], BATCH : [0/234], loss D : -10.80487060546875, loss G : -459.59259033203125\n",
      "EPOCH : [26/200], BATCH : [100/234], loss D : -13.00714111328125, loss G : -373.99493408203125\n",
      "EPOCH : [26/200], BATCH : [200/234], loss D : -1.03863525390625, loss G : -390.15008544921875\n",
      "EPOCH : [27/200], BATCH : [0/234], loss D : -6.712677001953125, loss G : -283.57672119140625\n",
      "EPOCH : [27/200], BATCH : [100/234], loss D : -8.85906982421875, loss G : -321.72998046875\n",
      "EPOCH : [27/200], BATCH : [200/234], loss D : -5.466339111328125, loss G : -422.29791259765625\n",
      "EPOCH : [28/200], BATCH : [0/234], loss D : -6.881805419921875, loss G : -482.20965576171875\n",
      "EPOCH : [28/200], BATCH : [100/234], loss D : -7.142120361328125, loss G : -422.8055725097656\n",
      "EPOCH : [28/200], BATCH : [200/234], loss D : -1.9932861328125, loss G : -412.0512390136719\n",
      "EPOCH : [29/200], BATCH : [0/234], loss D : -3.196563720703125, loss G : -290.16357421875\n",
      "EPOCH : [29/200], BATCH : [100/234], loss D : 1.3662109375, loss G : -441.61212158203125\n",
      "EPOCH : [29/200], BATCH : [200/234], loss D : 0.889801025390625, loss G : -304.8907165527344\n",
      "EPOCH : [30/200], BATCH : [0/234], loss D : 7.4306640625, loss G : -533.513671875\n",
      "EPOCH : [30/200], BATCH : [100/234], loss D : 5.04315185546875, loss G : -330.5057678222656\n",
      "EPOCH : [30/200], BATCH : [200/234], loss D : -8.586181640625, loss G : -437.0033264160156\n",
      "EPOCH : [31/200], BATCH : [0/234], loss D : 12.466461181640625, loss G : -442.3973388671875\n",
      "EPOCH : [31/200], BATCH : [100/234], loss D : -6.62078857421875, loss G : -573.0020751953125\n",
      "EPOCH : [31/200], BATCH : [200/234], loss D : -5.9373779296875, loss G : -505.1845703125\n",
      "EPOCH : [32/200], BATCH : [0/234], loss D : 0.2947998046875, loss G : -427.58673095703125\n",
      "EPOCH : [32/200], BATCH : [100/234], loss D : -17.971649169921875, loss G : -371.38409423828125\n",
      "EPOCH : [32/200], BATCH : [200/234], loss D : -12.043701171875, loss G : -390.89971923828125\n",
      "EPOCH : [33/200], BATCH : [0/234], loss D : -6.7620849609375, loss G : -439.8853759765625\n",
      "EPOCH : [33/200], BATCH : [100/234], loss D : -10.51129150390625, loss G : -442.581298828125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-c9d81d37f1d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss_D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_preds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgene_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss_D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rlawjdghek\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rlawjdghek\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        optimizer_D.zero_grad()\n",
    "        real_img = img.to(device)\n",
    "        latent_z = torch.nn.init.normal_(torch.zeros(BATCH_SIZE, LATENT_DIM)).to(device)\n",
    "        gene_img = generator(latent_z)\n",
    "        real_preds = discriminator(real_img)\n",
    "        gene_preds = discriminator(gene_img.detach())\n",
    "        \n",
    "        loss_D = -torch.mean(real_preds) + torch.mean(gene_preds)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "        if i % N_CRITIC == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            gene_img = generator(latent_z)\n",
    "            gene_preds = discriminator(gene_img)\n",
    "            loss_G = -torch.mean(gene_preds)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "        if i % 100==0:\n",
    "            print(\"EPOCH : [{}/{}], BATCH : [{}/{}], loss D : {}, loss G : {}\".format(epoch, NUM_EPOCHS, i, len(train_loader), loss_D, loss_G))\n",
    "            \n",
    "    if epoch % 10 ==0:\n",
    "        if not os.path.isdir(\"./result\"):\n",
    "            print(\"make ./result\")\n",
    "            os.makedirs(\"./result\")\n",
    "        torchvision.utils.save_image(gene_img[:16], \"./result/epoch{:d}.png\".format(epoch), nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlawjdghek",
   "language": "python",
   "name": "rlawjdghek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
